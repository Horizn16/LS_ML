{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade --quiet transformers datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:19:53.169497Z","iopub.execute_input":"2025-07-09T16:19:53.170346Z","iopub.status.idle":"2025-07-09T16:19:57.238320Z","shell.execute_reply.started":"2025-07-09T16:19:53.170323Z","shell.execute_reply":"2025-07-09T16:19:57.237588Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the WikiText-2 dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:20:44.395428Z","iopub.execute_input":"2025-07-09T16:20:44.395730Z","iopub.status.idle":"2025-07-09T16:20:47.162415Z","shell.execute_reply.started":"2025-07-09T16:20:44.395704Z","shell.execute_reply":"2025-07-09T16:20:47.161306Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['text'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['text'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 3760\n    })\n})\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n# GPT-2 does not have a pad token by default, so we add one\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:20:47.163491Z","iopub.execute_input":"2025-07-09T16:20:47.163829Z","iopub.status.idle":"2025-07-09T16:20:51.616454Z","shell.execute_reply.started":"2025-07-09T16:20:47.163793Z","shell.execute_reply":"2025-07-09T16:20:51.615758Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\nprint(tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:20:51.617347Z","iopub.execute_input":"2025-07-09T16:20:51.617897Z","iopub.status.idle":"2025-07-09T16:20:57.993530Z","shell.execute_reply.started":"2025-07-09T16:20:51.617866Z","shell.execute_reply":"2025-07-09T16:20:57.992708Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b966f6ed521440aa04a1ef4b858c2f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f71dc1db294f0b8eee2d5a657230b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e77fa55620934d25bea7394e50ef878f"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 4358\n    })\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 36718\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 3760\n    })\n})\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"### Model selection","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:20:57.995064Z","iopub.execute_input":"2025-07-09T16:20:57.995261Z","iopub.status.idle":"2025-07-09T16:20:57.998403Z","shell.execute_reply.started":"2025-07-09T16:20:57.995247Z","shell.execute_reply":"2025-07-09T16:20:57.997854Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel\n\n# Load the pretrained GPT-2 model\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n# Align the model's padding token with the tokenizer\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:20:57.999329Z","iopub.execute_input":"2025-07-09T16:20:57.999758Z","iopub.status.idle":"2025-07-09T16:21:01.471702Z","shell.execute_reply.started":"2025-07-09T16:20:57.999731Z","shell.execute_reply":"2025-07-09T16:21:01.471116Z"}},"outputs":[{"name":"stderr","text":"2025-07-09 16:20:58.856747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752078058.875792     138 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752078058.882513     138 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\n# Split the dataset\ntrain_dataset = tokenized_datasets[\"train\"]\neval_dataset = tokenized_datasets[\"validation\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:21:01.472503Z","iopub.execute_input":"2025-07-09T16:21:01.473054Z","iopub.status.idle":"2025-07-09T16:21:02.075414Z","shell.execute_reply.started":"2025-07-09T16:21:01.473027Z","shell.execute_reply":"2025-07-09T16:21:02.074871Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    save_total_limit=1,\n    logging_steps=10\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T16:33:22.747905Z","iopub.execute_input":"2025-07-09T16:33:22.748216Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Save model and tokenizer to a directory\noutput_dir = \"./gpt2-finetuned-wikitext2\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\neval_results = trainer.evaluate()\neval_loss = eval_results[\"eval_loss\"]\nperplexity = math.exp(eval_loss)\nprint(f\"Perplexity: {perplexity:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef compute_top_k_accuracy(model, dataset, k=5):\n    model.eval()\n    correct = 0\n    total = 0\n    for batch in torch.utils.data.DataLoader(dataset, batch_size=8):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            # Get the logits for the last token in each sequence\n            last_token_logits = logits[:, -2, :]  # -2 because -1 is usually padding/eos\n            next_token = input_ids[:, -1]\n            top_k = torch.topk(last_token_logits, k, dim=-1).indices\n            for i in range(next_token.size(0)):\n                if next_token[i] in top_k[i]:\n                    correct += 1\n                total += 1\n    return correct / total\n\ntop5_acc = compute_top_k_accuracy(model, eval_dataset, k=5)\nprint(f\"Top-5 Accuracy: {top5_acc:.2%}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}